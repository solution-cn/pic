# pic
Quadrotor_VC_Parl
====
基于Parl框架：利用Model、Algorithm、Agent以及其内部已集成的算法DDPG复现rlschool内的Quadrotor速度控制任务。 
在实践过程中发现不管如何调参或者调整Action的训练策略（4个电机单独控制抑或采用5个参数分别表示基础电压以及四个电机的Offset）都轻易能够将reward下调至-21左右。但在render观察实测效果发现这个任务并没有很好地完成，实际上无人机只学会了“省电”（实际上是学会减少各个电机的电压变化）。
个人经过对rlschool中Quadrotor环境相关的代码进行研究，发现内部设置的reward是由“寿命”、“目标”、“状态”组成。其中关于寿命的指标是一致的，经过测试，是指无人机电机电压变化的快慢，突变得越快，值越高，最低由内部参数healthy_reward控制，默认为1，取前者乘以系数dt(0.01)和healthy_reward最小值，再取  **负**  作为关于寿命的reward部分。
而对于目标部分，只采用了实际速度与目标速度的三维相差的绝对值之和，而且乘以系数-0.001。无人机速度三维里最多-10到10，这意味着即使是相差得最大，60*-0.001=-0.06的reward，与电机突变，四个电机从0.1V到15V造成的-22.7(2270+)最后取-1相比孰轻孰重无人机当然学得会。
实际上只需要将每次的动作(action)设置成常量不变就能够取得-21左右的reward。
也就是rlschool内Quadrotor环境中velocity_control任务中原设的reward不能指引Agent完成工作的目标。因此必须重新设计能够表示速度控制这一任务的reward供Agent参考训练。
----
据此考虑了几种方式：
1.采用原reward与xyz实际速度与目标速度的方差之和相结合：
reward += k * ((vx-t_vx)^2 + (vy-t_vy)^2 + (vz-t_vz)^2)
k取0.01
2.剔除寿命相关的reward，外部计算xyz实际速度与目标速度的方差之和与xyz实际速度与目标速度的差的绝对值之和（考虑到速度差接近1以内时后者比前者更适合作为reward）：
reward = k *  ((vx-t_vx)^2 + (vy-t_vy)^2 + (vz-t_vz)^2 + |vx-t_vx| + |vy-t_vy| + |vz-t_vz|
k取0.1
最后均在测试时计算该环境任务内计算的原reward进行观察。
----
